# 大数据最新2021年面试题，高级面试题及附答案解析

### 其实，博主还整理了，更多大厂面试题，直接下载吧

### 下载链接：[高清172份，累计 7701 页大厂面试题  PDF](https://github.com/souyunku/DevBooks/blob/master/docs/index.md)



### 1、腾讯面试题：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？

与上第6题类似，我的第一反应时快速排序+二分查找。以下是其它更好的方法：

方案1：oo，申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。

方案2：这个问题在《编程珠玑》里有很好的描述，大家可以参考下面的思路，探讨一下：

又因为2^32为40亿多，所以给定一个数可能在，也可能不在其中；

这里我们把40亿个数中的每一个用32位的二进制来表示

假设这40亿个数开始放在一个文件中。

然后将这40亿个数分成两类:

1.最高位为0

2.最高位为1

并将这两类分别写入到两个文件中，其中一个文件中数的个数<=20亿，而另一个>=20亿（这相当于折半了）；

与要查找的数的最高位比较并接着进入相应的文件再查找

再然后把这个文件为又分成两类:

1.次最高位为0

2.次最高位为1

并将这两类分别写入到两个文件中，其中一个文件中数的个数<=10亿，而另一个>=10亿（这相当于折半了）；

与要查找的数的次最高位比较并接着进入相应的文件再查找。

…….

以此类推，就可以找到了,而且时间复杂度为O(logn)，方案2完。

附：这里，再简单介绍下，位图方法：

使用位图法判断整形数组是否存在重复

判断集合中存在重复是常见编程任务之一，当集合中数据量比较大时我们通常希望少进行几次扫描，这时双重循环法就不可取了。

位图法比较适合于这种情况，它的做法是按照集合中最大元素max创建一个长度为max+1的新数组，然后再次扫描原数组，遇到几就给新数组的第几位置上1，如遇到5就给新数组的第六个元素置1，这样下次再遇到5想置位时发现新数组的第六个元素已经是1了，这说明这次的数据肯定和以前的数据存在着重复。这种给新数组初始化时置零其后置一的做法类似于位图的处理方法故称位图法。它的运算次数最坏的情况为2N。如果已知数组的最大值即能事先给新数组定长的话效率还能提高一倍。

欢迎，有更好的思路，或方法，共同交流。


### 2、简述hive中的虚拟列的作用？使用它注意事项

三个虚拟列

INPUT_FILE_NAME：mapper任务的输出文件名

BLOCK_OFFSET_INSIDE_FILE：当前全局文件偏移量，对于快压缩文件，就是当前快的文件偏移量，及当前块的第一个字节在文件中的偏移量

ROW_OFFSET_INSIDE_BLOCK，默认不开启，设置hive.exec.rowoffset为true可用，可以用来排查有问题的输入数据


### 3、如何确定hadoop集群的健康状态

UI监控 shell监控


### 4、假设公司要建一个数据中心，你会如何处理？

先进行需求调查分析

设计功能划分

架构设计

吞吐量的估算

采用的技术类型

软硬件选型

成本效益的分析

项目管理

扩展性

安全性，稳定性


### 5、我们开发Job是否能去掉reduce阶段

可以去掉。设置reduce数为0即可


### 6、hive sql知识点

DML 数据操纵语言

DDL 数据定义语言，用语定义和管理数据库中的对象


### 7、请你用最熟悉的语言编写mapreduce，计算第四列每个元素出现的个数



```
public classWordCount1 {

public static final String INPUT_PATH ="hdfs://hadoop0:9000/in";

public static final String OUT_PATH ="hdfs://hadoop0:9000/out";

public static void main(String[] args)throws Exception {

      Configuration conf = newConfiguration();

      FileSystem fileSystem =FileSystem.get(conf);

      if(fileSystem.exists(newPath(OUT_PATH))){}

      fileSystem.delete(newPath(OUT_PATH),true);

      Job job = newJob(conf,WordCount1.class.getSimpleName());

      //1.0读取文件，解析成key,value对

      FileInputFormat.setInputPaths(job,newPath(INPUT_PATH));

      //2.0写上自己的逻辑，对输入的可以，value进行处理，转换成新的key,value对进行输出

      job.setMapperClass(MyMapper.class);

      job.setMapOutputKeyClass(Text.class);

      job.setMapOutputValueClass(LongWritable.class);

      //3.0对输出后的数据进行分区

      //4.0对分区后的数据进行排序，分组，相同key的value放到一个集合中

      //5.0对分组后的数据进行规约

      //6.0对通过网络将map输出的数据拷贝到reduce节点

      //7.0 写上自己的reduce函数逻辑，对map输出的数据进行处理

      job.setReducerClass(MyReducer.class);

      job.setOutputKeyClass(Text.class);

      job.setOutputValueClass(LongWritable.class);

      FileOutputFormat.setOutputPath(job,new Path(OUT_PATH));

      job.waitForCompletion(true);

}

static class MyMapper extendsMapper<LongWritable, Text, Text, LongWritable>{

      @Override

      protected void map(LongWritablek1, Text v1,

                    org.apache.hadoop.mapreduce.Mapper.Contextcontext)

                    throws IOException,InterruptedException {

             String[] split =v1.toString().split("\t");

             for(String words :split){

                    context.write(split[3],1);

             }

      }

}

static class MyReducer extends Reducer<Text,LongWritable, Text, LongWritable>{



      protected void reduce(Text k2,Iterable<LongWritable> v2,

                    org.apache.hadoop.mapreduce.Reducer.Contextcontext)

                    throws IOException,InterruptedException {

             Long count = 0L;

             for(LongWritable time :v2){

                    count += time.get();

             }

             context.write(v2, newLongWritable(count));

      }

}

}
```


### 8、hbase如何调优

**1、** 垃圾回收调优

**2、** 优化region拆分合并以及拆分region

**3、** 客户端入库调优

**4、** Hbase配置文件


### 9、请简述hadoop怎样实现二级排序（就是对key和value双排序）

第一种方法是，Reducer将给定key的所有值都缓存起来，然后对它们再做一个Reducer内排序。但是，由于Reducer需要保存给定key的所有值，可能会导致出现内存耗尽的错误。

第二种方法是，将值的一部分或整个值加入原始key，生成一个组合key。这两种方法各有优势，第一种方法编写简单，但并发度小，数据量大的情况下速度慢(有内存耗尽的危险)，

第二种方法则是将排序的任务交给MapReduce框架shuffle，更符合Hadoop/Reduce的设计思想。这篇文章里选择的是第二种。我们将编写一个Partitioner，确保拥有相同key(原始key，不包括添加的部分)的所有数据被发往同一个Reducer，还将编写一个Comparator，以便数据到达Reducer后即按原始key分组。

68.简述hadoop实现jion的几种方法

Map side join----大小表join的场景，可以借助distributed cache

Reduce side join


### 10、hive为何分区

避免数据倾斜，查询效率提升


### 11、请列出你所知道的hadoop调度器，并简要说明其工作方法？
### 12、为什么要用flume导入hdfs，hdfs的架构是怎样的？
### 13、offset是每天消息的偏移量
### 14、简答说一下hadoop的map-reduce编程模型
### 15、Hive生产环境中为什么建议使用外部表？
### 16、insert into 和override write区别
### 17、请列出你在工作中使用过的开发mapreduce的语言
### 18、举一个例子说明mapreduce是怎么运行的。
### 19、怎么在海量数据中找出重复次数最多的一个？
### 20、3个datanode中有一个datanode出现错误会怎样？
### 21、Hive与关系型数据库的关系？
### 22、在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。
### 23、hdfs-site.xml的3个主要属性？
### 24、你们数据库如何导入hive的出现什么错误
### 25、什么是Kafka？
### 26、spark调优
### 27、Hadoop是否遵循UNIX模式？
### 28、启动和关闭命令会用到哪些文件？
### 29、hbase内部机制是什么
### 30、Kafka怎么保证消息不丢失机制
### 31、hive数仓开发的基本流程
### 32、请用java实现非递归二分查询
### 33、HBase写数据的原理是什么？




## 全部答案，整理好了，直接下载吧

### 下载链接：[全部答案，整理好了](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin-2.png)




## 最新，高清PDF：172份，7701页，最新整理

[![大厂面试题](https://www.souyunku.com/wp-content/uploads/weixin/mst.png "架构师专栏")](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")

[![大厂面试题](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")
